"""
SQLAlchemy ORM Models for PostgreSQL Authentication Database.

Maps to the PostgreSQL schema defined in src/scripts/sql/schema.sql.
Uses SQLAlchemy 2.0 declarative syntax with proper relationships and indexes.

Created: 2025-11-17
Database: lupin_auth
"""

from sqlalchemy import (
    Column,
    String,
    Boolean,
    DateTime,
    ForeignKey,
    BigInteger,
    Text,
    Index,
    func
)
from sqlalchemy.dialects.postgresql import UUID, JSONB, INET
from sqlalchemy.orm import DeclarativeBase, relationship, Mapped, mapped_column
from datetime import datetime
from typing import Optional, List
import uuid


class Base( DeclarativeBase ):
    """Base class for all ORM models."""
    pass


class User( Base ):
    """
    User account model.

    Requires:
        - email: Valid unique email address
        - password_hash: Hashed password (never store plaintext)

    Ensures:
        - id is automatically generated UUID
        - created_at defaults to current timestamp
        - roles defaults to ["user"]
        - relationships cascade delete to tokens and keys
    """
    __tablename__ = "users"

    # Primary Key
    id: Mapped[uuid.UUID] = mapped_column(
        UUID( as_uuid=True ),
        primary_key=True,
        default=uuid.uuid4,
        server_default=func.gen_random_uuid()
    )

    # User Credentials
    email: Mapped[str] = mapped_column(
        String( 255 ),
        unique=True,
        nullable=False,
        index=True
    )
    password_hash: Mapped[str] = mapped_column(
        String( 255 ),
        nullable=False
    )

    # Account Status
    email_verified: Mapped[bool] = mapped_column(
        Boolean,
        default=False,
        server_default="false"
    )
    is_active: Mapped[bool] = mapped_column(
        Boolean,
        default=True,
        server_default="true",
        index=True
    )

    # User Metadata
    roles: Mapped[dict] = mapped_column(
        JSONB,
        default=["user"],
        server_default='["user"]'
    )
    created_at: Mapped[datetime] = mapped_column(
        DateTime( timezone=True ),
        nullable=False,
        default=func.now(),
        server_default=func.now()
    )
    last_login_at: Mapped[Optional[datetime]] = mapped_column(
        DateTime( timezone=True ),
        nullable=True
    )

    # Relationships (cascade delete to dependent records)
    refresh_tokens: Mapped[List["RefreshToken"]] = relationship(
        "RefreshToken",
        back_populates="user",
        cascade="all, delete-orphan"
    )
    api_keys: Mapped[List["ApiKey"]] = relationship(
        "ApiKey",
        back_populates="user",
        cascade="all, delete-orphan"
    )
    email_verification_tokens: Mapped[List["EmailVerificationToken"]] = relationship(
        "EmailVerificationToken",
        back_populates="user",
        cascade="all, delete-orphan"
    )
    password_reset_tokens: Mapped[List["PasswordResetToken"]] = relationship(
        "PasswordResetToken",
        back_populates="user",
        cascade="all, delete-orphan"
    )
    notifications: Mapped[List["Notification"]] = relationship(
        "Notification",
        back_populates="recipient",
        cascade="all, delete-orphan"
    )

    # Indexes defined in schema
    __table_args__ = (
        Index( 'idx_users_email', 'email' ),
        Index( 'idx_users_is_active', 'is_active' ),
        Index( 'idx_users_roles', 'roles', postgresql_using='gin' ),
    )

    def __repr__( self ) -> str:
        return f"<User(id={self.id}, email='{self.email}', active={self.is_active})>"


class RefreshToken( Base ):
    """
    Refresh token model for JWT authentication.

    Requires:
        - user_id: Valid user UUID
        - token_hash: SHA-256 hash of refresh token
        - expires_at: Token expiration timestamp

    Ensures:
        - jti (JWT ID) is automatically generated UUID
        - created_at defaults to current timestamp
        - revoked defaults to False
        - cascades delete when user is deleted
    """
    __tablename__ = "refresh_tokens"

    # Primary Key (JWT ID)
    jti: Mapped[uuid.UUID] = mapped_column(
        UUID( as_uuid=True ),
        primary_key=True,
        default=uuid.uuid4,
        server_default=func.gen_random_uuid()
    )

    # Foreign Key to User
    user_id: Mapped[uuid.UUID] = mapped_column(
        UUID( as_uuid=True ),
        ForeignKey( "users.id", ondelete="CASCADE" ),
        nullable=False,
        index=True
    )

    # Token Data
    token_hash: Mapped[str] = mapped_column(
        String( 64 ),
        nullable=False,
        index=True
    )
    expires_at: Mapped[datetime] = mapped_column(
        DateTime( timezone=True ),
        nullable=False,
        index=True
    )
    revoked: Mapped[bool] = mapped_column(
        Boolean,
        default=False,
        server_default="false"
    )

    # Metadata
    created_at: Mapped[datetime] = mapped_column(
        DateTime( timezone=True ),
        nullable=False,
        default=func.now(),
        server_default=func.now()
    )
    last_used_at: Mapped[Optional[datetime]] = mapped_column(
        DateTime( timezone=True ),
        nullable=True
    )
    user_agent: Mapped[Optional[str]] = mapped_column(
        Text,
        nullable=True
    )
    ip_address: Mapped[Optional[str]] = mapped_column(
        INET,
        nullable=True
    )

    # Relationship to User
    user: Mapped["User"] = relationship(
        "User",
        back_populates="refresh_tokens"
    )

    # Indexes defined in schema
    __table_args__ = (
        Index( 'idx_refresh_tokens_user_id', 'user_id' ),
        Index( 'idx_refresh_tokens_expires_at', 'expires_at' ),
        Index( 'idx_refresh_tokens_token_hash', 'token_hash' ),
        Index(
            'idx_refresh_tokens_revoked',
            'revoked',
            postgresql_where=(Column( 'revoked' ) == False)
        ),
    )

    def __repr__( self ) -> str:
        return f"<RefreshToken(jti={self.jti}, user_id={self.user_id}, revoked={self.revoked})>"


class ApiKey( Base ):
    """
    API key model for service account authentication.

    Requires:
        - user_id: Valid user UUID
        - key_hash: SHA-256 hash of API key

    Ensures:
        - id is automatically generated UUID
        - created_at defaults to current timestamp
        - is_active defaults to True
        - cascades delete when user is deleted
    """
    __tablename__ = "api_keys"

    # Primary Key
    id: Mapped[uuid.UUID] = mapped_column(
        UUID( as_uuid=True ),
        primary_key=True,
        default=uuid.uuid4,
        server_default=func.gen_random_uuid()
    )

    # Foreign Key to User
    user_id: Mapped[uuid.UUID] = mapped_column(
        UUID( as_uuid=True ),
        ForeignKey( "users.id", ondelete="CASCADE" ),
        nullable=False,
        index=True
    )

    # Key Data
    key_hash: Mapped[str] = mapped_column(
        String( 64 ),
        nullable=False,
        index=True
    )
    description: Mapped[Optional[str]] = mapped_column(
        Text,
        nullable=True
    )
    is_active: Mapped[bool] = mapped_column(
        Boolean,
        default=True,
        server_default="true",
        index=True
    )

    # Metadata
    created_at: Mapped[datetime] = mapped_column(
        DateTime( timezone=True ),
        nullable=False,
        default=func.now(),
        server_default=func.now()
    )
    last_used_at: Mapped[Optional[datetime]] = mapped_column(
        DateTime( timezone=True ),
        nullable=True
    )

    # Relationship to User
    user: Mapped["User"] = relationship(
        "User",
        back_populates="api_keys"
    )

    # Indexes defined in schema
    __table_args__ = (
        Index( 'idx_api_keys_key_hash', 'key_hash' ),
        Index( 'idx_api_keys_user_id', 'user_id' ),
        Index( 'idx_api_keys_is_active', 'is_active' ),
    )

    def __repr__( self ) -> str:
        return f"<ApiKey(id={self.id}, user_id={self.user_id}, active={self.is_active})>"


class EmailVerificationToken( Base ):
    """
    Email verification token model.

    Requires:
        - token: Unique verification token string
        - user_id: Valid user UUID
        - expires_at: Token expiration timestamp

    Ensures:
        - created_at defaults to current timestamp
        - used defaults to False
        - cascades delete when user is deleted
    """
    __tablename__ = "email_verification_tokens"

    # Primary Key
    token: Mapped[str] = mapped_column(
        String( 255 ),
        primary_key=True
    )

    # Foreign Key to User
    user_id: Mapped[uuid.UUID] = mapped_column(
        UUID( as_uuid=True ),
        ForeignKey( "users.id", ondelete="CASCADE" ),
        nullable=False,
        index=True
    )

    # Token Data
    expires_at: Mapped[datetime] = mapped_column(
        DateTime( timezone=True ),
        nullable=False
    )
    used: Mapped[bool] = mapped_column(
        Boolean,
        default=False,
        server_default="false"
    )
    created_at: Mapped[datetime] = mapped_column(
        DateTime( timezone=True ),
        nullable=False,
        default=func.now(),
        server_default=func.now()
    )

    # Relationship to User
    user: Mapped["User"] = relationship(
        "User",
        back_populates="email_verification_tokens"
    )

    # Indexes defined in schema
    __table_args__ = (
        Index( 'idx_email_verification_user_id', 'user_id' ),
    )

    def __repr__( self ) -> str:
        return f"<EmailVerificationToken(token={self.token[:8]}..., user_id={self.user_id}, used={self.used})>"


class PasswordResetToken( Base ):
    """
    Password reset token model.

    Requires:
        - token: Unique reset token string
        - user_id: Valid user UUID
        - expires_at: Token expiration timestamp

    Ensures:
        - created_at defaults to current timestamp
        - used defaults to False
        - cascades delete when user is deleted
    """
    __tablename__ = "password_reset_tokens"

    # Primary Key
    token: Mapped[str] = mapped_column(
        String( 255 ),
        primary_key=True
    )

    # Foreign Key to User
    user_id: Mapped[uuid.UUID] = mapped_column(
        UUID( as_uuid=True ),
        ForeignKey( "users.id", ondelete="CASCADE" ),
        nullable=False,
        index=True
    )

    # Token Data
    expires_at: Mapped[datetime] = mapped_column(
        DateTime( timezone=True ),
        nullable=False
    )
    used: Mapped[bool] = mapped_column(
        Boolean,
        default=False,
        server_default="false"
    )
    created_at: Mapped[datetime] = mapped_column(
        DateTime( timezone=True ),
        nullable=False,
        default=func.now(),
        server_default=func.now()
    )

    # Relationship to User
    user: Mapped["User"] = relationship(
        "User",
        back_populates="password_reset_tokens"
    )

    # Indexes defined in schema
    __table_args__ = (
        Index( 'idx_password_reset_user_id', 'user_id' ),
    )

    def __repr__( self ) -> str:
        return f"<PasswordResetToken(token={self.token[:8]}..., user_id={self.user_id}, used={self.used})>"


class FailedLoginAttempt( Base ):
    """
    Failed login attempt model for security monitoring.

    Requires:
        - email: Email address used in login attempt
        - ip_address: IP address of login attempt

    Ensures:
        - id is automatically generated BIGSERIAL
        - attempt_time defaults to current timestamp
        - no foreign key (tracks attempts for non-existent users too)
    """
    __tablename__ = "failed_login_attempts"

    # Primary Key (BIGSERIAL auto-increment)
    id: Mapped[int] = mapped_column(
        BigInteger,
        primary_key=True,
        autoincrement=True
    )

    # Attempt Data
    email: Mapped[str] = mapped_column(
        String( 255 ),
        nullable=False,
        index=True
    )
    ip_address: Mapped[str] = mapped_column(
        INET,
        nullable=False
    )
    attempt_time: Mapped[datetime] = mapped_column(
        DateTime( timezone=True ),
        nullable=False,
        default=func.now(),
        server_default=func.now(),
        index=True
    )

    # Indexes defined in schema
    __table_args__ = (
        Index( 'idx_failed_login_email', 'email' ),
        Index( 'idx_failed_login_attempt_time', 'attempt_time' ),
    )

    def __repr__( self ) -> str:
        return f"<FailedLoginAttempt(id={self.id}, email='{self.email}', ip={self.ip_address})>"


class Notification( Base ):
    """
    Notification model for sender-aware notification system.

    Requires:
        - sender_id: Sender identifier (e.g., claude.code@lupin.deepily.ai)
        - recipient_id: Valid user UUID
        - message: Notification message text

    Ensures:
        - id is automatically generated UUID
        - created_at defaults to current timestamp
        - state defaults to 'created'
        - cascades delete when recipient user is deleted
    """
    __tablename__ = "notifications"

    # Primary Key
    id: Mapped[uuid.UUID] = mapped_column(
        UUID( as_uuid=True ),
        primary_key=True,
        default=uuid.uuid4,
        server_default=func.gen_random_uuid()
    )

    # Routing - sender_id is the key field for multi-project grouping
    sender_id: Mapped[str] = mapped_column(
        String( 255 ),
        nullable=False,
        index=True
    )
    recipient_id: Mapped[uuid.UUID] = mapped_column(
        UUID( as_uuid=True ),
        ForeignKey( "users.id", ondelete="CASCADE" ),
        nullable=False,
        index=True
    )
    job_id: Mapped[Optional[str]] = mapped_column(
        String( 256 ),
        nullable=True,
        index=True
    )  # Agentic job ID for routing - compound IDs: SHA256(64) + '::' + UUID(36) = 102+ chars
    progress_group_id: Mapped[Optional[str]] = mapped_column(
        String( 24 ),
        nullable=True,
        index=True
    )  # Progress group ID for in-place DOM updates (format: pg-{hex} or pr-{hex}-{batch})

    # Content
    title: Mapped[Optional[str]] = mapped_column(
        String( 255 ),
        nullable=True
    )
    message: Mapped[str] = mapped_column(
        Text,
        nullable=False
    )
    abstract: Mapped[Optional[str]] = mapped_column(
        Text,
        nullable=True
    )
    type: Mapped[str] = mapped_column(
        String( 50 ),
        nullable=False,
        index=True
    )
    priority: Mapped[str] = mapped_column(
        String( 50 ),
        nullable=False
    )

    # Timestamps
    created_at: Mapped[datetime] = mapped_column(
        DateTime( timezone=True ),
        nullable=False,
        default=func.now(),
        server_default=func.now()
    )
    delivered_at: Mapped[Optional[datetime]] = mapped_column(
        DateTime( timezone=True ),
        nullable=True
    )
    responded_at: Mapped[Optional[datetime]] = mapped_column(
        DateTime( timezone=True ),
        nullable=True
    )
    expires_at: Mapped[Optional[datetime]] = mapped_column(
        DateTime( timezone=True ),
        nullable=True
    )

    # Response handling
    response_requested: Mapped[bool] = mapped_column(
        Boolean,
        default=False,
        server_default="false"
    )
    response_type: Mapped[Optional[str]] = mapped_column(
        String( 50 ),
        nullable=True
    )
    response_value: Mapped[Optional[dict]] = mapped_column(
        JSONB,
        nullable=True
    )
    response_default: Mapped[Optional[str]] = mapped_column(
        String( 255 ),
        nullable=True
    )
    response_options: Mapped[Optional[dict]] = mapped_column(
        JSONB,
        nullable=True
    )
    timeout_seconds: Mapped[Optional[int]] = mapped_column(
        BigInteger,
        nullable=True
    )

    # State machine (created, queued, delivered, responded, expired, error)
    state: Mapped[str] = mapped_column(
        String( 50 ),
        nullable=False,
        default="created",
        server_default="created",
        index=True
    )

    # Soft delete / archive flag for hiding notifications without deletion
    is_hidden: Mapped[bool] = mapped_column(
        Boolean,
        default=False,
        server_default="false",
        index=True
    )

    # Relationship to User
    recipient: Mapped["User"] = relationship(
        "User",
        back_populates="notifications"
    )

    # Indexes
    __table_args__ = (
        Index( 'idx_notifications_sender_id', 'sender_id' ),
        Index( 'idx_notifications_recipient_id', 'recipient_id' ),
        Index( 'idx_notifications_state', 'state' ),
        Index( 'idx_notifications_created_at', 'created_at' ),
        Index( 'idx_notifications_sender_recipient', 'sender_id', 'recipient_id' ),
    )

    def __repr__( self ) -> str:
        return f"<Notification(id={self.id}, sender='{self.sender_id}', state='{self.state}')>"


class AuthAuditLog( Base ):
    """
    Authentication audit log model for security tracking.

    Requires:
        - event_type: Type of authentication event

    Ensures:
        - id is automatically generated BIGSERIAL
        - event_time defaults to current timestamp
        - success defaults to True
        - user_id is optional (tracks events for non-authenticated users)
    """
    __tablename__ = "auth_audit_log"

    # Primary Key (BIGSERIAL auto-increment)
    id: Mapped[int] = mapped_column(
        BigInteger,
        primary_key=True,
        autoincrement=True
    )

    # Event Data
    event_type: Mapped[str] = mapped_column(
        String( 50 ),
        nullable=False,
        index=True
    )
    user_id: Mapped[Optional[uuid.UUID]] = mapped_column(
        UUID( as_uuid=True ),
        nullable=True,
        index=True
    )
    email: Mapped[Optional[str]] = mapped_column(
        String( 255 ),
        nullable=True
    )
    ip_address: Mapped[Optional[str]] = mapped_column(
        INET,
        nullable=True
    )
    details: Mapped[Optional[dict]] = mapped_column(
        JSONB,
        nullable=True
    )
    success: Mapped[bool] = mapped_column(
        Boolean,
        default=True,
        server_default="true"
    )
    event_time: Mapped[datetime] = mapped_column(
        DateTime( timezone=True ),
        nullable=False,
        default=func.now(),
        server_default=func.now(),
        index=True
    )

    # Indexes defined in schema
    __table_args__ = (
        Index( 'idx_auth_audit_event_type', 'event_type' ),
        Index( 'idx_auth_audit_user_id', 'user_id' ),
        Index( 'idx_auth_audit_event_time', 'event_time' ),
    )

    def __repr__( self ) -> str:
        return f"<AuthAuditLog(id={self.id}, event_type='{self.event_type}', user_id={self.user_id}, success={self.success})>"


# ============================================================================
# Decision Proxy Models (Phase 4)
# ============================================================================

class ProxyDecision( Base ):
    """
    Decision proxy decision log — every decision classified and acted on (or shadowed).

    Requires:
        - notification_id: UUID of the original notification
        - domain: Domain identifier (e.g., "swe", "devops")
        - category: Decision category within the domain
        - question: Original question text
        - action: Action taken (shadow, suggest, act, defer)

    Ensures:
        - id is automatically generated UUID
        - created_at defaults to current timestamp
        - ratification_state defaults to "not_required"
        - metadata stored as JSONB for extensibility
    """
    __tablename__ = "proxy_decisions"

    # Primary Key
    id: Mapped[uuid.UUID] = mapped_column(
        UUID( as_uuid=True ),
        primary_key=True,
        default=uuid.uuid4,
        server_default=func.gen_random_uuid()
    )

    # Decision context
    notification_id: Mapped[str] = mapped_column(
        String( 255 ),
        nullable=False,
        index=True
    )
    domain: Mapped[str] = mapped_column(
        String( 50 ),
        nullable=False,
        index=True
    )
    category: Mapped[str] = mapped_column(
        String( 100 ),
        nullable=False,
        index=True
    )
    question: Mapped[str] = mapped_column(
        Text,
        nullable=False
    )
    sender_id: Mapped[Optional[str]] = mapped_column(
        String( 255 ),
        nullable=True,
        index=True
    )

    # Decision outcome
    action: Mapped[str] = mapped_column(
        String( 50 ),
        nullable=False,
        index=True
    )
    decision_value: Mapped[Optional[str]] = mapped_column(
        Text,
        nullable=True
    )
    confidence: Mapped[Optional[float]] = mapped_column(
        nullable=True
    )
    trust_level: Mapped[int] = mapped_column(
        nullable=False,
        default=1
    )
    reason: Mapped[Optional[str]] = mapped_column(
        Text,
        nullable=True
    )

    # Ratification
    ratification_state: Mapped[str] = mapped_column(
        String( 50 ),
        nullable=False,
        default="not_required",
        server_default="not_required",
        index=True
    )
    ratified_by: Mapped[Optional[str]] = mapped_column(
        String( 255 ),
        nullable=True
    )
    ratified_at: Mapped[Optional[datetime]] = mapped_column(
        DateTime( timezone=True ),
        nullable=True
    )
    ratification_feedback: Mapped[Optional[str]] = mapped_column(
        Text,
        nullable=True
    )

    # Extensible metadata
    metadata_json: Mapped[Optional[dict]] = mapped_column(
        JSONB,
        nullable=True
    )

    # Data provenance (organic, synthetic_seed, synthetic_generated)
    data_origin: Mapped[str] = mapped_column(
        String( 50 ),
        nullable=False,
        default="organic",
        server_default="organic",
        index=True
    )

    # Timestamps
    created_at: Mapped[datetime] = mapped_column(
        DateTime( timezone=True ),
        nullable=False,
        default=func.now(),
        server_default=func.now()
    )

    # Indexes
    __table_args__ = (
        Index( 'idx_proxy_decisions_domain', 'domain' ),
        Index( 'idx_proxy_decisions_category', 'category' ),
        Index( 'idx_proxy_decisions_action', 'action' ),
        Index( 'idx_proxy_decisions_ratification', 'ratification_state' ),
        Index( 'idx_proxy_decisions_created_at', 'created_at' ),
        Index( 'idx_proxy_decisions_domain_category', 'domain', 'category' ),
        Index( 'idx_proxy_decisions_data_origin', 'data_origin' ),
    )

    def __repr__( self ) -> str:
        return f"<ProxyDecision(id={self.id}, domain='{self.domain}', category='{self.category}', action='{self.action}')>"


class TrustState( Base ):
    """
    Trust state snapshot — per-user, per-domain, per-category trust tracking.

    Stores the current trust level and aggregate decision statistics for
    a specific domain+category combination. Updated after each ratification.

    Requires:
        - user_email: User email for multi-user trust isolation
        - domain: Domain identifier
        - category: Decision category

    Ensures:
        - id is automatically generated UUID
        - updated_at tracks last modification
        - circuit_breaker_state stored as JSONB
    """
    __tablename__ = "trust_states"

    # Primary Key
    id: Mapped[uuid.UUID] = mapped_column(
        UUID( as_uuid=True ),
        primary_key=True,
        default=uuid.uuid4,
        server_default=func.gen_random_uuid()
    )

    # Trust context
    user_email: Mapped[str] = mapped_column(
        String( 255 ),
        nullable=False,
        index=True
    )
    domain: Mapped[str] = mapped_column(
        String( 50 ),
        nullable=False,
        index=True
    )
    category: Mapped[str] = mapped_column(
        String( 100 ),
        nullable=False,
        index=True
    )

    # Trust metrics
    trust_level: Mapped[int] = mapped_column(
        nullable=False,
        default=1
    )
    total_decisions: Mapped[int] = mapped_column(
        nullable=False,
        default=0
    )
    successful_decisions: Mapped[int] = mapped_column(
        nullable=False,
        default=0
    )
    rejected_decisions: Mapped[int] = mapped_column(
        nullable=False,
        default=0
    )

    # Circuit breaker
    circuit_breaker_state: Mapped[Optional[dict]] = mapped_column(
        JSONB,
        nullable=True
    )

    # Timestamps
    created_at: Mapped[datetime] = mapped_column(
        DateTime( timezone=True ),
        nullable=False,
        default=func.now(),
        server_default=func.now()
    )
    updated_at: Mapped[datetime] = mapped_column(
        DateTime( timezone=True ),
        nullable=False,
        default=func.now(),
        server_default=func.now()
    )

    # Indexes
    __table_args__ = (
        Index( 'idx_trust_states_user_domain', 'user_email', 'domain' ),
        Index( 'idx_trust_states_category', 'category' ),
        Index( 'idx_trust_states_user_domain_category', 'user_email', 'domain', 'category', unique=True ),
    )

    def __repr__( self ) -> str:
        return f"<TrustState(user={self.user_email}, domain='{self.domain}', category='{self.category}', level={self.trust_level})>"


class PredictionLog( Base ):
    """
    Prediction log — tracks prediction engine predictions alongside actual human responses.

    Each row represents one prediction attempt for a notification. The predicted_value
    is filled at prediction time; actual_value and accuracy fields are filled when
    the human responds.

    Requires:
        - notification_id references a valid notifications row
        - response_type is a valid response type string
        - category is a non-empty string

    Ensures:
        - id is automatically generated UUID
        - predicted_at defaults to current timestamp
        - accuracy_match is null until outcome is recorded
    """
    __tablename__ = "prediction_log"

    # Primary Key
    id: Mapped[uuid.UUID] = mapped_column(
        UUID( as_uuid=True ),
        primary_key=True,
        default=uuid.uuid4,
        server_default=func.gen_random_uuid()
    )

    # Link to notification
    notification_id: Mapped[uuid.UUID] = mapped_column(
        UUID( as_uuid=True ),
        ForeignKey( "notifications.id", ondelete="CASCADE" ),
        nullable=False,
        index=True
    )

    # Classification context
    response_type: Mapped[str] = mapped_column(
        String( 50 ),
        nullable=False,
        index=True
    )
    category: Mapped[str] = mapped_column(
        String( 100 ),
        nullable=False,
        index=True
    )

    # Prediction output
    predicted_value: Mapped[Optional[dict]] = mapped_column(
        JSONB,
        nullable=True
    )
    prediction_confidence: Mapped[float] = mapped_column(
        nullable=False,
        default=0.0
    )
    prediction_strategy: Mapped[str] = mapped_column(
        String( 50 ),
        nullable=False
    )
    similar_case_count: Mapped[int] = mapped_column(
        nullable=False,
        default=0
    )

    # Actual outcome (filled on response)
    actual_value: Mapped[Optional[dict]] = mapped_column(
        JSONB,
        nullable=True
    )
    accuracy_match: Mapped[Optional[bool]] = mapped_column(
        nullable=True,
        index=True
    )
    accuracy_detail: Mapped[Optional[dict]] = mapped_column(
        JSONB,
        nullable=True
    )

    # Timestamps
    predicted_at: Mapped[datetime] = mapped_column(
        DateTime( timezone=True ),
        nullable=False,
        default=func.now(),
        server_default=func.now()
    )
    responded_at: Mapped[Optional[datetime]] = mapped_column(
        DateTime( timezone=True ),
        nullable=True
    )

    # Sender context
    sender_id: Mapped[Optional[str]] = mapped_column(
        String( 255 ),
        nullable=True,
        index=True
    )

    # Indexes
    __table_args__ = (
        Index( 'idx_prediction_log_response_type_category', 'response_type', 'category' ),
        Index( 'idx_prediction_log_predicted_at', 'predicted_at' ),
    )

    def __repr__( self ) -> str:
        return f"<PredictionLog(id={self.id}, type='{self.response_type}', category='{self.category}', match={self.accuracy_match})>"


def quick_smoke_test():
    """
    Quick smoke test for postgres_models module - validates PostgreSQL ORM model definitions.
    """
    import cosa.utils.util as cu

    cu.print_banner( "PostgreSQL SQLAlchemy ORM Models Smoke Test", prepend_nl=True )

    try:
        # Test 1: Module imports
        print( "Testing module imports..." )
        from sqlalchemy import create_engine
        from sqlalchemy.orm import sessionmaker
        print( "✓ SQLAlchemy imports successful" )

        # Test 2: Model classes exist
        print( "Testing model class definitions..." )
        models = [User, RefreshToken, ApiKey, EmailVerificationToken,
                  PasswordResetToken, FailedLoginAttempt, Notification, AuthAuditLog,
                  ProxyDecision, TrustState, PredictionLog]
        for model in models:
            assert hasattr( model, '__tablename__' ), f"{model.__name__} missing __tablename__"
        print( f"✓ All {len( models )} models defined: {', '.join( [m.__name__ for m in models] )}" )

        # Test 3: Base metadata exists
        print( "Testing Base metadata..." )
        assert hasattr( Base, 'metadata' ), "Base missing metadata"
        table_names = list( Base.metadata.tables.keys() )
        expected_tables = ['users', 'refresh_tokens', 'api_keys', 'email_verification_tokens',
                          'password_reset_tokens', 'failed_login_attempts', 'notifications', 'auth_audit_log',
                          'proxy_decisions', 'trust_states', 'prediction_log']
        assert set( table_names ) == set( expected_tables ), f"Table mismatch: {table_names}"
        print( f"✓ Base metadata contains {len( table_names )} tables" )

        # Test 4: Relationships defined
        print( "Testing ORM relationships..." )
        assert hasattr( User, 'refresh_tokens' ), "User missing refresh_tokens relationship"
        assert hasattr( User, 'api_keys' ), "User missing api_keys relationship"
        assert hasattr( User, 'notifications' ), "User missing notifications relationship"
        assert hasattr( RefreshToken, 'user' ), "RefreshToken missing user relationship"
        assert hasattr( Notification, 'recipient' ), "Notification missing recipient relationship"
        print( "✓ ORM relationships defined correctly" )

        # Test 5: Test PostgreSQL connection and schema validation
        print( "Testing PostgreSQL connection and schema validation..." )
        import os
        from sqlalchemy import text
        db_url = os.environ.get( 'DATABASE_URL', 'postgresql://lupin_dev:dev_password@localhost:5432/lupin_auth' )
        try:
            engine = create_engine( db_url )
            # Test connection
            with engine.connect() as conn:
                result = conn.execute( text( "SELECT 1" ) )
                result.close()
            print( f"✓ PostgreSQL connection successful" )

            # Compare ORM metadata with actual database schema
            from sqlalchemy import inspect
            inspector = inspect( engine )
            db_tables = inspector.get_table_names()
            expected = set( expected_tables )
            actual = set( db_tables )

            if expected.issubset( actual ):
                print( f"✓ All {len( expected_tables )} ORM tables exist in database" )
            else:
                missing = expected - actual
                print( f"⚠ Warning: Tables missing from database: {missing}" )

        except Exception as db_error:
            print( f"⚠ PostgreSQL connection skipped: {db_error}" )
            print( "  (This is OK if database is not running)" )

        print( "\n✓ Smoke test completed successfully" )

    except Exception as e:
        print( f"\n✗ Smoke test failed: {e}" )
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    quick_smoke_test()
