#!/usr/bin/env python3
"""
Runtime Argument Expeditor - Core Logic.

Sits between LORA intent classification and agentic job creation.
When a user says "make me a podcast", the LORA model identifies the routing
command but may not capture all required arguments. The expeditor detects
missing args via LLM gap analysis against --help output, then asks the user
for missing information via synchronous voice notifications.

Scope: Deep Research, Podcast Generator, Research-to-Podcast (3 agents).
"""

import json
import os
import re
from typing import Optional

import cosa.utils.util as cu

from cosa.agents.runtime_argument_expeditor.agent_registry import (
    AGENTIC_AGENTS,
    get_cli_help,
    get_user_visible_args
)
from cosa.agents.runtime_argument_expeditor.xml_models import ExpeditorResponse, ArgConfirmationResponse
from cosa.agents.llm_client_factory import LlmClientFactory
from cosa.agents.io_models.utils.prompt_template_processor import PromptTemplateProcessor
from lupin_cli.notifications.notify_user_sync import notify_user_sync
from lupin_cli.notifications.notification_models import (
    NotificationRequest,
    NotificationPriority,
    ResponseType
)
from cosa.utils.notification_utils import (
    format_open_ended_batch_for_tts,
    convert_open_ended_batch_for_api
)


class RuntimeArgumentExpeditor:
    """
    Determines which required arguments a user's voice command provides and
    asks for any missing ones before creating an agentic job.

    Requires:
        - config_mgr is a valid ConfigurationManager instance
        - The config has keys: runtime argument expeditor enabled,
          llm spec key for runtime argument expeditor,
          prompt template for runtime argument expeditor

    Ensures:
        - expedite() returns a complete args dict or None (cancel/timeout)
        - Uses LLM only for gap analysis; questions come from static registry
        - System-provided args are never asked for
    """

    SENDER_ID = "arg.expeditor@lupin.deepily.ai"

    def __init__( self, config_mgr, debug=False, verbose=False ):
        """
        Initialize the runtime argument expeditor.

        Requires:
            - config_mgr is a valid ConfigurationManager instance

        Ensures:
            - Reads 3 config keys
            - Creates LlmClientFactory singleton reference

        Args:
            config_mgr: ConfigurationManager instance
            debug: Enable debug output
            verbose: Enable verbose output
        """
        self.config_mgr                = config_mgr
        self.debug                     = debug
        self.verbose                   = verbose
        self.llm_spec_key              = config_mgr.get( "llm spec key for runtime argument expeditor" )
        self.prompt_template_path      = config_mgr.get( "prompt template for runtime argument expeditor" )
        self.confirmation_prompt_path  = config_mgr.get( "prompt template for argument confirmation" )
        self.llm_factory               = LlmClientFactory( debug=debug, verbose=verbose )
        self._last_notification_status = None

    def expedite( self, command, raw_args, user_email, session_id, user_id, original_question, job_id=None, bearer_token=None ):
        """
        Run argument gap analysis and collect missing arguments from user.

        Requires:
            - command is a key in AGENTIC_AGENTS
            - raw_args is a string (may be empty)
            - user_email, session_id, user_id are non-empty strings
            - original_question is the full voice command string

        Ensures:
            - Returns dict of complete args if all required args are satisfied
            - Returns None if user cancels, times out, or command not found
            - System-provided args are injected, never asked for

        Args:
            command: Routing command key (e.g., "agent router go to deep research")
            raw_args: LORA-extracted arguments string
            user_email: Authenticated user's email
            session_id: WebSocket session ID
            user_id: System user ID
            original_question: Full voice command transcription
            job_id: Optional agentic job ID for routing notifications to job cards
            bearer_token: Optional JWT for authenticating notification requests

        Returns:
            dict or None: Complete argument dictionary or None on cancel
        """
        self._job_id       = job_id
        self._bearer_token = bearer_token

        agent_entry = AGENTIC_AGENTS.get( command )
        if not agent_entry:
            print( f"[Expeditor] Unknown command: {command}" )
            return None

        # Step 1: Capture --help output
        help_text = get_cli_help( command )
        if not help_text:
            help_text = "(CLI help not available)"

        # Step 2: Parse LORA args and map to CLI names
        lora_args    = self._parse_lora_args( raw_args )
        mapped_args  = {}
        arg_mapping  = agent_entry[ "arg_mapping" ]

        for lora_name, value in lora_args.items():
            cli_name = arg_mapping.get( lora_name, lora_name )
            mapped_args[ cli_name ] = value

        if self.debug:
            print( f"[Expeditor] LORA args: {lora_args}" )
            print( f"[Expeditor] Mapped args: {mapped_args}" )

        # Step 3: Build and run LLM gap analysis prompt
        template_raw = cu.get_file_as_string(
            cu.get_project_root() + self.prompt_template_path
        )

        # Process {{PYDANTIC_XML_EXAMPLE}} first
        processor = PromptTemplateProcessor( debug=self.debug )
        template_processed = processor.process_template(
            template_raw, "runtime argument expeditor"
        )

        # Fill runtime placeholders
        system_args   = ", ".join( agent_entry[ "system_provided" ] )
        required_args = ", ".join( agent_entry[ "required_user_args" ] )
        extracted_str = ", ".join( f"{k}={v}" for k, v in mapped_args.items() ) if mapped_args else "(none)"

        prompt = template_processed.format(
            system_args    = system_args,
            help_text      = help_text,
            voice_command  = original_question,
            extracted_args = extracted_str,
            required_args  = required_args
        )

        if self.debug and self.verbose:
            print( f"[Expeditor] Prompt ({len( prompt )} chars):\n{prompt[ :500 ]}..." )

        # Step 4: Call LLM
        llm_client = self.llm_factory.get_client(
            self.llm_spec_key, debug=self.debug, verbose=self.verbose
        )
        response = llm_client.run( prompt )

        if self.debug: print( f"[Expeditor] LLM response: {response[ :300 ]}" )

        # Step 5: Parse response
        try:
            parsed = ExpeditorResponse.from_xml( response )
        except Exception as e:
            print( f"[Expeditor] Failed to parse LLM response: {e}" )
            # Fallback: assume all required args are missing
            parsed = ExpeditorResponse(
                all_required_met = "false",
                args_present     = "",
                args_missing     = ", ".join( agent_entry[ "required_user_args" ] )
            )

        # Step 6: Merge LLM-detected present args with LORA-mapped args
        present_dict = parsed.get_present_dict()

        final_args = dict( mapped_args )
        for k, v in present_dict.items():
            if k not in final_args:
                final_args[ k ] = v

        if self.debug: print( f"[Expeditor] Args after LLM merge: {final_args}" )

        # Step 7: Compute missing user-visible args deterministically
        # This replaces the old if/else gate on parsed.is_complete() which
        # skipped optional arg prompting when all required args were present.
        user_visible       = get_user_visible_args( command )
        fallback_questions = agent_entry[ "fallback_questions" ]
        fallback_defaults  = agent_entry.get( "fallback_defaults", {} )
        special_handlers   = agent_entry.get( "special_handlers", {} )

        # Fallback: if CLI doesn't publish user-visible-args, use fallback_questions keys
        if user_visible is None:
            user_visible = list( fallback_questions.keys() )

        # Missing = user-visible args not yet in final_args
        missing = [ arg for arg in user_visible if arg not in final_args ]

        # Pre-populate fallback defaults for required args that have no default.
        # The user's original question is the best default for "task" or "query" fields.
        required_args = agent_entry[ "required_user_args" ]
        for arg_name in required_args:
            if arg_name in missing and arg_name not in fallback_defaults:
                fallback_defaults[ arg_name ] = original_question

        if self.debug: print( f"[Expeditor] Missing user-visible args: {missing}" )

        if missing:
            # Partition missing args into batchable and special
            batchable = []
            special   = []
            for arg_name in missing:
                if special_handlers.get( arg_name ):
                    special.append( arg_name )
                else:
                    batchable.append( arg_name )

            # Build request context abstract for notification UI
            request_abstract = self._build_request_context(
                agent_entry, original_question, final_args, batchable + special
            )

            # Batch-collect batchable args if more than one
            if len( batchable ) > 1:
                if self.debug: print( f"[Expeditor] Batch-collecting {len( batchable )} args: {batchable}" )
                batch_answers = self._batch_collect_args( batchable, fallback_questions, user_email, fallback_defaults, command, abstract=request_abstract )
                if batch_answers is None:
                    print( "[Expeditor] User cancelled batch collection" )
                    return None
                for arg_name, value in batch_answers.items():
                    # Handle special "no limit" / "none" answers for optional args
                    if arg_name in ( "budget", "languages", "timeout" ) and value.lower().strip() in ( "no limit", "none", "skip", "no", "default" ):
                        continue
                    final_args[ arg_name ] = value
            elif len( batchable ) == 1:
                # Single arg — use existing sequential flow
                arg_name = batchable[ 0 ]
                resolved_default = self._resolve_default( command, arg_name, fallback_defaults.get( arg_name ) )
                if arg_name in fallback_questions:
                    value = self._ask_for_arg( arg_name, fallback_questions[ arg_name ], user_email, response_default=resolved_default, abstract=request_abstract )
                else:
                    value = self._ask_for_arg( arg_name, f"Please provide the '{arg_name}' argument.", user_email, response_default=resolved_default, abstract=request_abstract )
                if value is None:
                    print( f"[Expeditor] User cancelled at arg '{arg_name}'" )
                    return None
                if arg_name in ( "budget", "languages", "timeout" ) and value.lower().strip() in ( "no limit", "none", "skip", "no", "default" ):
                    pass  # Skip optional
                else:
                    final_args[ arg_name ] = value

            # Handle special args sequentially (e.g., fuzzy_file_match)
            for arg_name in special:
                handler = special_handlers[ arg_name ]
                if handler == "fuzzy_file_match":
                    value = self._handle_fuzzy_file_match( user_email )
                else:
                    value = self._ask_for_arg( arg_name, f"Please provide the '{arg_name}' argument.", user_email, abstract=request_abstract )
                if value is None:
                    print( f"[Expeditor] User cancelled at arg '{arg_name}'" )
                    return None
                final_args[ arg_name ] = value

        if self.debug: print( f"[Expeditor] Final args: {final_args}" )

        # Step 8: Confirmation loop — user reviews args before submission
        confirmed_args = self._confirm_and_iterate( final_args, agent_entry, command, user_email )
        if confirmed_args is None:
            print( "[Expeditor] User cancelled during confirmation" )
            return None

        return self._inject_system_args(
            confirmed_args, agent_entry, user_email, session_id, user_id
        )

    def _confirm_and_iterate( self, args_dict, agent_entry, command_key, user_email ):
        """
        Present argument summary and iterate until user approves, modifies, or cancels.

        Uses YES_NO prompt type. Summary is shown via abstract (not spoken).
        Spoken message is a clean question. User comments carry modification intent.

        Requires:
            - args_dict contains all collected user-facing args
            - agent_entry is the registry entry for this agent
            - command_key is the agent's key in AGENTIC_AGENTS

        Ensures:
            - Returns approved args_dict, or None if cancelled
            - User has seen and approved all arguments before submission
            - Only user-visible args are shown (whitelist from CLI)
            - Maximum 5 iterations to prevent infinite loops
            - "yes" approves, "no" cancels
            - "yes [comment: ...]" applies tweak then proceeds
            - "no [comment: ...]" applies tweak then re-presents

        Args:
            args_dict: Collected argument dictionary
            agent_entry: Registry entry for the target agent
            command_key: Key in AGENTIC_AGENTS for user-visible-args lookup
            user_email: Target user for voice prompts

        Returns:
            dict or None: Approved args_dict or None on cancel
        """
        max_iterations = 5

        # Whitelist: only show user-visible args in confirmation summary
        user_visible = get_user_visible_args( command_key )
        # Fallback: if agent doesn't publish, use fallback_questions keys
        if user_visible is None:
            user_visible = list( agent_entry.get( "fallback_questions", {} ).keys() )

        for iteration in range( max_iterations ):
            # Build summary of user-visible args only → abstract (shown, not spoken)
            summary_lines = []
            for k, v in args_dict.items():
                if k in user_visible:
                    summary_lines.append( f"- **{k}**: {v}" )

            agent_name = agent_entry.get( "display_name", agent_entry[ "cli_module" ].split( "." )[ -1 ].replace( "_", " " ) )

            abstract = f"**{agent_name} Job Summary**\n\n" + "\n".join( summary_lines )
            message  = f"Here's what I have for your {agent_name} job. Does this look right?"

            response = self._ask_for_confirmation( message, user_email, abstract=abstract )

            if response is None:
                return None

            lower = response.lower().strip()

            # Plain "yes" or "no" (no comment)
            if lower == "yes":
                return args_dict

            if lower == "no":
                return None

            # "yes [comment: ...]" or "no [comment: ...]"
            comment = self._extract_comment( response )

            if lower.startswith( "yes" ):
                if comment:
                    modification = self._parse_modification( comment, args_dict, agent_entry )
                    if modification and modification.is_modify() and modification.arg_name and modification.new_value:
                        args_dict[ modification.arg_name ] = modification.new_value
                        if self.debug: print( f"  Modified: {modification.arg_name} = {modification.new_value}" )
                    # User said "yes" — proceed even if parse failed
                return args_dict

            if lower.startswith( "no" ):
                if comment:
                    modification = self._parse_modification( comment, args_dict, agent_entry )
                    if modification and modification.is_modify() and modification.arg_name and modification.new_value:
                        args_dict[ modification.arg_name ] = modification.new_value
                        if self.debug: print( f"  Modified: {modification.arg_name} = {modification.new_value}" )
                        # Loop continues — re-present updated summary
                        continue
                # No comment or parse failed — respect the "no"
                return None

        # Safety valve: too many iterations
        if self.debug: print( "[Expeditor] Max confirmation iterations reached, proceeding" )
        return args_dict

    def _parse_modification( self, user_response, args_dict, agent_entry ):
        """
        Use LLM to parse a user's modification intent from their voice response.

        Requires:
            - user_response is a non-empty string
            - args_dict contains current arguments
            - agent_entry contains the agent registry entry

        Ensures:
            - Returns ArgConfirmationResponse on successful parse
            - Returns None on parse failure

        Args:
            user_response: The user's voice response text
            args_dict: Current argument dictionary
            agent_entry: Registry entry for context

        Returns:
            ArgConfirmationResponse or None
        """
        try:
            template_raw = cu.get_file_as_string(
                cu.get_project_root() + self.confirmation_prompt_path
            )

            processor = PromptTemplateProcessor( debug=self.debug )
            template_processed = processor.process_template(
                template_raw, "argument confirmation"
            )

            # Build current args summary and arg names list
            system_args = set( agent_entry.get( "system_provided", [] ) )
            current_args_str = ", ".join(
                f"{k}={v}" for k, v in args_dict.items()
                if k not in system_args and k != "no_confirm"
            )
            arg_names_str = ", ".join(
                k for k in args_dict.keys()
                if k not in system_args and k != "no_confirm"
            )

            # Also include fallback question keys as valid arg names
            fallback_keys = ", ".join( agent_entry.get( "fallback_questions", {} ).keys() )
            if fallback_keys:
                arg_names_str = arg_names_str + ", " + fallback_keys if arg_names_str else fallback_keys

            prompt = template_processed.format(
                user_response = user_response,
                current_args  = current_args_str,
                arg_names     = arg_names_str
            )

            if self.debug and self.verbose:
                print( f"[Expeditor] Confirmation prompt ({len( prompt )} chars):\n{prompt[ :300 ]}..." )

            llm_client = self.llm_factory.get_client(
                self.llm_spec_key, debug=self.debug, verbose=self.verbose
            )
            response = llm_client.run( prompt )

            if self.debug: print( f"[Expeditor] Confirmation LLM response: {response[ :200 ]}" )

            parsed = ArgConfirmationResponse.from_xml( response )
            return parsed

        except Exception as e:
            print( f"[Expeditor] Failed to parse confirmation response: {e}" )
            return None

    def _inject_system_args( self, args_dict, agent_entry, user_email, session_id, user_id ):
        """
        Inject system-provided arguments into the args dictionary.

        Requires:
            - args_dict is a dict of user-provided arguments
            - agent_entry has "system_provided" list

        Ensures:
            - Returns args_dict with system args injected
            - Does not overwrite existing user-provided values

        Returns:
            dict: Args dict with system args added
        """
        system_map = {
            "user_email"  : user_email,
            "session_id"  : session_id,
            "user_id"     : user_id,
            "no_confirm"  : True,
        }

        for sys_arg in agent_entry[ "system_provided" ]:
            if sys_arg in system_map and sys_arg not in args_dict:
                args_dict[ sys_arg ] = system_map[ sys_arg ]

        return args_dict

    def _parse_lora_args( self, raw_args_str ):
        """
        Parse LORA raw argument string into a dictionary.

        Handles formats:
            - key="value"
            - key='value'
            - key=value (no quotes, stops at whitespace or comma)

        Requires:
            - raw_args_str is a string (may be empty or None)

        Ensures:
            - Returns dict mapping arg names to values
            - Handles multiple formats gracefully

        Args:
            raw_args_str: Raw argument string from LORA router

        Returns:
            dict: Parsed argument name-value pairs
        """
        if not raw_args_str or not raw_args_str.strip():
            return {}

        result = {}

        # Match key="value", key='value', or key=value patterns
        pattern = r'(\w+)\s*=\s*(?:"([^"]*?)"|\'([^\']*?)\'|(\S+))'
        matches = re.findall( pattern, raw_args_str )

        for match in matches:
            key = match[ 0 ]
            # Value is in whichever capture group matched
            value = match[ 1 ] or match[ 2 ] or match[ 3 ]
            result[ key ] = value

        return result

    def _resolve_default( self, command_key, arg_name, registry_default ):
        """
        Resolve default value for an argument: config override > registry > None.

        Requires:
            - command_key is a key in AGENTIC_AGENTS
            - arg_name is the CLI argument name
            - registry_default is the fallback_defaults value (or None)

        Ensures:
            - Returns config override if present, else registry default, else None

        Args:
            command_key: Routing command key (e.g., "agent router go to deep research")
            arg_name: CLI argument name (e.g., "budget")
            registry_default: Default from agent_registry fallback_defaults

        Returns:
            str or None: Resolved default value
        """
        agent_short = command_key.replace( "agent router go to ", "" )
        config_key  = f"expeditor default value for {agent_short} {arg_name}"
        config_value = self.config_mgr.get( config_key, default=None )
        if config_value is not None:
            return config_value
        return registry_default

    def _build_request_context( self, agent_entry, original_question, final_args, missing_args ):
        """
        Build a markdown abstract summarizing the current request context.

        Displayed alongside batch question forms in the notification UI (not spoken via TTS).
        Helps the user understand why they're being asked and what the system already knows.

        Requires:
            - agent_entry is a valid registry entry with 'display_name'
            - original_question is the user's voice command string
            - final_args is a dict of already-extracted arguments
            - missing_args is a list of arg names still needed

        Ensures:
            - Returns a markdown string with request context
            - Only includes user-visible args in "Already extracted" section
            - Conditionally includes sections only when non-empty

        Args:
            agent_entry: Registry entry for the target agent
            original_question: Full voice command transcription
            final_args: Dict of already-resolved arguments
            missing_args: List of arg names still needed

        Returns:
            str: Markdown-formatted context summary
        """
        display_name = agent_entry.get( "display_name", agent_entry[ "cli_module" ].split( "." )[ -1 ].replace( "_", " " ) )

        lines = [
            f'**Your request**: "{original_question}"',
            f"**Agent**: {display_name}",
        ]

        # Filter present args to user-visible only
        user_visible = get_user_visible_args(
            next( ( k for k, v in AGENTIC_AGENTS.items() if v is agent_entry ), None )
        )
        system_args = set( agent_entry.get( "system_provided", [] ) )

        visible_present = {
            k: v for k, v in final_args.items()
            if k not in system_args
            and k != "no_confirm"
            and ( user_visible is None or k in user_visible )
        }

        if visible_present:
            lines.append( "" )
            lines.append( "**Already extracted**:" )
            for k, v in visible_present.items():
                lines.append( f"- {k}: {v}" )

        if missing_args:
            lines.append( "" )
            lines.append( "**Still needed**: " + ", ".join( missing_args ) )

        return "\n".join( lines )

    def _ask_for_arg( self, arg_name, question, user_email, response_default=None, abstract=None ):
        """
        Ask the user for a missing argument via synchronous notification.

        Requires:
            - arg_name is a non-empty string
            - question is the question text to ask
            - user_email is the target user's email

        Ensures:
            - Returns user's response string on success
            - Returns None on timeout, error, or cancellation

        Args:
            arg_name: Name of the missing argument
            question: Fallback question from registry
            user_email: Target user for notification
            response_default: Optional pre-filled default value for the input
            abstract: Optional markdown context shown in UI but not spoken

        Returns:
            str or None: User's response or None
        """
        request = NotificationRequest(
            message          = question,
            response_type    = ResponseType.OPEN_ENDED,
            priority         = NotificationPriority.HIGH,
            target_user      = user_email,
            timeout_seconds  = 180,
            sender_id        = self.SENDER_ID,
            title            = f"Missing: {arg_name}",
            suppress_ding    = False,
            response_default = response_default,
            abstract         = abstract,
            job_id           = self._job_id
        )

        response = notify_user_sync( request=request, debug=self.debug, bearer_token=self._bearer_token )
        self._last_notification_status = response.status

        if self.debug:
            print( f"[Expeditor] _ask_for_arg response: success={response.success}, status={response.status}, "
                   f"exit_code={response.exit_code}, is_timeout={response.is_timeout}, "
                   f"value={response.response_value[ :100 ] if response.response_value else None}" )

        if response.success and response.response_value:
            value = response.response_value.strip()
            # Check for cancellation keywords
            if value.lower() in ( "cancel", "nevermind", "never mind", "stop", "quit" ):
                return None
            return value

        return None

    def _ask_for_confirmation( self, message, user_email, abstract=None ):
        """
        Ask the user a YES_NO confirmation question via synchronous notification.

        Requires:
            - message is a non-empty string (spoken via TTS)
            - user_email is the target user's email

        Ensures:
            - Returns raw response string ("yes", "no", "yes [comment: ...]", "no [comment: ...]")
            - Returns None on timeout or error

        Args:
            message: The confirmation question to speak
            user_email: Target user for notification
            abstract: Optional markdown context shown in UI but not spoken

        Returns:
            str or None: Raw response string or None
        """
        request = NotificationRequest(
            message          = message,
            response_type    = ResponseType.YES_NO,
            priority         = NotificationPriority.HIGH,
            target_user      = user_email,
            timeout_seconds  = 180,
            sender_id        = self.SENDER_ID,
            title            = "Confirm",
            suppress_ding    = False,
            response_default = "no",
            abstract         = abstract,
            job_id           = self._job_id
        )

        response = notify_user_sync( request=request, debug=self.debug, bearer_token=self._bearer_token )
        self._last_notification_status = response.status

        if self.debug:
            print( f"[Expeditor] _ask_for_confirmation response: success={response.success}, status={response.status}, "
                   f"exit_code={response.exit_code}, is_timeout={response.is_timeout}, "
                   f"value={response.response_value[ :100 ] if response.response_value else None}" )

        if response.success and response.response_value:
            return response.response_value.strip()

        return None

    @staticmethod
    def _extract_comment( response_text ):
        """
        Extract comment text from a YES_NO response with annotation.

        Requires:
            - response_text is a string like "yes [comment: change budget to 10]"

        Ensures:
            - Returns the comment string if pattern matches
            - Returns None if no comment found

        Args:
            response_text: Raw YES_NO response string

        Returns:
            str or None: Extracted comment text or None
        """
        match = re.search( r'\[comment:\s*(.+?)\]', response_text )
        return match.group( 1 ).strip() if match else None

    def _batch_collect_args( self, batchable_args, fallback_questions, user_email, fallback_defaults=None, command_key=None, abstract=None ):
        """
        Collect multiple missing arguments in a single batch notification.

        Sends all questions at once via OPEN_ENDED_BATCH notification type.
        User sees all questions on one screen and submits all answers together.
        When defaults are available, text inputs are pre-filled so the user
        can accept by simply hitting Submit All.

        Requires:
            - batchable_args is a list of arg names (len > 1)
            - fallback_questions maps arg names to question strings
            - user_email is the target user's email

        Ensures:
            - Returns dict of { arg_name: value } on success
            - Returns None on timeout, error, or cancellation
            - Questions include default_value when resolved default is not None

        Args:
            batchable_args: List of arg names to collect
            fallback_questions: Dict mapping arg names to question strings
            user_email: Target user for notification
            fallback_defaults: Optional dict mapping arg names to default values
            command_key: Optional routing command key for config override lookup
            abstract: Optional markdown context shown in UI but not spoken

        Returns:
            dict or None: Collected answers or None on cancel/timeout
        """
        if fallback_defaults is None:
            fallback_defaults = {}

        questions = []
        for arg_name in batchable_args:
            question_text = fallback_questions.get(
                arg_name, f"Please provide the '{arg_name}' argument."
            )
            q = {
                "question" : question_text,
                "header"   : arg_name
            }
            # Resolve default: config override > registry > None
            resolved_default = self._resolve_default(
                command_key, arg_name, fallback_defaults.get( arg_name )
            ) if command_key else fallback_defaults.get( arg_name )
            if resolved_default is not None:
                q[ "default_value" ] = resolved_default
            questions.append( q )

        tts_message      = format_open_ended_batch_for_tts( questions )
        response_options = convert_open_ended_batch_for_api( questions )

        request = NotificationRequest(
            message          = tts_message,
            response_type    = ResponseType.OPEN_ENDED_BATCH,
            priority         = NotificationPriority.HIGH,
            target_user      = user_email,
            timeout_seconds  = 300,
            sender_id        = self.SENDER_ID,
            title            = "Missing arguments",
            response_options = response_options,
            suppress_ding    = False,
            abstract         = abstract,
            job_id           = self._job_id
        )

        response = notify_user_sync( request=request, debug=self.debug, bearer_token=self._bearer_token )
        self._last_notification_status = response.status

        if self.debug:
            print( f"[Expeditor] _batch_collect_args response: success={response.success}, status={response.status}, "
                   f"exit_code={response.exit_code}, is_timeout={response.is_timeout}, "
                   f"value={response.response_value[ :100 ] if response.response_value else None}" )

        if not response.success or not response.response_value:
            return None

        # Parse JSON response
        try:
            parsed = json.loads( response.response_value )
        except ( json.JSONDecodeError, TypeError ):
            if self.debug: print( f"[Expeditor] Failed to parse batch response: {response.response_value}" )
            return None

        # Check for cancellation
        if parsed.get( "cancelled" ):
            return None

        answers = parsed.get( "answers", {} )
        if not answers:
            return None

        # Check for cancellation keywords in any answer
        for arg_name, value in answers.items():
            if isinstance( value, str ) and value.lower().strip() in ( "cancel", "nevermind", "never mind", "stop", "quit" ):
                return None

        # Check all requested args have non-empty values
        for arg_name in batchable_args:
            if arg_name not in answers or not str( answers.get( arg_name, "" ) ).strip():
                if self.debug: print( f"[Expeditor] Batch response missing arg: {arg_name}" )
                return None

        return answers

    def _handle_fuzzy_file_match( self, user_email ):
        """
        Use fuzzy file matching to find a document by user description.

        Searches the user's deep research directory AND additional directories
        from the 'podcast generator source search paths' config key.

        Requires:
            - user_email is a valid email

        Ensures:
            - Returns full file path if user selects a match
            - Returns None if no matches found or user cancels

        Args:
            user_email: User's email (determines research directory)

        Returns:
            str or None: Full path to selected document
        """
        from cosa.agents.io_models.xml_models import FuzzyFileMatchResponse
        from cosa.config.configuration_manager import ConfigurationManager

        config_mgr   = ConfigurationManager( env_var_name="LUPIN_CONFIG_MGR_CLI_ARGS" )
        project_root = cu.get_project_root()

        # Build docs_map: { relative_path → abs_path } from all search dirs
        docs_map = {}

        # Source 1: Deep research directory
        research_dir = project_root + f"/io/deep-research/{user_email}"
        if os.path.exists( research_dir ):
            for f in os.listdir( research_dir ):
                if f.endswith( ".md" ):
                    rel_path = f"io/deep-research/{user_email}/{f}"
                    docs_map[ rel_path ] = f"{research_dir}/{f}"

        # Source 2: Additional search paths from config
        search_paths_raw = config_mgr.get( "podcast generator source search paths", default="/src" )
        search_dirs = [ d.strip() for d in search_paths_raw.split( "," ) if d.strip() ]

        for search_dir in search_dirs:
            abs_search_dir = project_root + search_dir
            if not os.path.exists( abs_search_dir ):
                if self.debug: print( f"[Expeditor] Search dir not found: {abs_search_dir}" )
                continue
            for root, _dirs, files in os.walk( abs_search_dir ):
                for f in files:
                    if f.endswith( ".md" ):
                        abs_path = os.path.join( root, f )
                        rel_path = os.path.relpath( abs_path, project_root )
                        if rel_path not in docs_map:
                            docs_map[ rel_path ] = abs_path

        if not docs_map:
            if self.debug: print( f"[Expeditor] No markdown files found in any search directory" )
            return self._ask_for_arg(
                "research",
                "No documents found. Please provide the path to a document.",
                user_email
            )

        # Ask user to describe which document
        description = self._ask_for_arg(
            "research",
            "Which document should I use for the podcast? Describe it or say the filename.",
            user_email
        )
        if not description:
            return None

        # Check if they gave an exact relative path or bare filename
        if description in docs_map:
            return docs_map[ description ]
        for rel_path, abs_path in docs_map.items():
            if os.path.basename( rel_path ) == description:
                return abs_path

        # Try fuzzy matching via LLM
        try:
            template_path = config_mgr.get( "prompt template for fuzzy file matching" )
            template = cu.get_file_as_string( project_root + template_path )

            processor = PromptTemplateProcessor( debug=self.debug )
            template = processor.process_template( template, "fuzzy file matching" )

            file_list = "\n".join( f"- {rel}" for rel in sorted( docs_map.keys() ) )
            prompt = template.format( description=description, file_list=file_list )

            llm_client = self.llm_factory.get_client(
                config_mgr.get( "llm spec key for fuzzy file matching" ),
                debug=self.debug, verbose=self.verbose
            )
            response = llm_client.run( prompt )

            parsed  = FuzzyFileMatchResponse.from_xml( response )
            raw_matches = parsed.get_matches_list()

            # Validate against docs_map (match relative paths or bare filenames)
            matches = []
            for m in raw_matches:
                if m in docs_map:
                    matches.append( m )
                else:
                    for rel_path in docs_map:
                        if os.path.basename( rel_path ) == m:
                            matches.append( rel_path )
                            break

            if not matches:
                if self.debug: print( "[Expeditor] No fuzzy matches found" )
                return self._ask_for_arg(
                    "research",
                    "I couldn't find a matching document. Please say the exact filename or path.",
                    user_email
                )

            if len( matches ) == 1:
                return docs_map[ matches[ 0 ] ]

            # Multiple matches - ask user to pick
            options_str = ", ".join( f"{i + 1}. {m}" for i, m in enumerate( matches ) )
            pick = self._ask_for_arg(
                "research",
                f"I found multiple matches: {options_str}. Say the number or name of the one you want.",
                user_email
            )
            if not pick:
                return None

            # Try to match by number
            try:
                idx = int( pick.strip() ) - 1
                if 0 <= idx < len( matches ):
                    return docs_map[ matches[ idx ] ]
            except ValueError:
                pass

            # Try to match by name
            for m in matches:
                if pick.lower().strip() in m.lower():
                    return docs_map[ m ]

            # Fallback: use first match
            return docs_map[ matches[ 0 ] ]

        except Exception as e:
            print( f"[Expeditor] Fuzzy match error: {e}" )
            return self._ask_for_arg(
                "research",
                "Matching failed. Please provide the exact filename or path.",
                user_email
            )


# ============================================================================
# Smoke Test
# ============================================================================

def quick_smoke_test():
    """
    Quick smoke test for RuntimeArgumentExpeditor.

    Tests imports, arg parsing, registry lookup, and help capture.
    Does NOT require a running server or LLM.
    """
    cu.print_banner( "Runtime Argument Expeditor Smoke Test", prepend_nl=True )

    tests_passed = 0
    tests_failed = 0

    # Test 1: Imports
    print( "\n1. Testing imports..." )
    try:
        from cosa.agents.runtime_argument_expeditor.agent_registry import AGENTIC_AGENTS, get_cli_help
        from cosa.agents.runtime_argument_expeditor.xml_models import ExpeditorResponse
        from cosa.agents.runtime_argument_expeditor.expeditor import RuntimeArgumentExpeditor
        print( "   ✓ All imports successful" )
        tests_passed += 1
    except Exception as e:
        print( f"   ✗ Import failed: {e}" )
        tests_failed += 1

    # Test 2: _parse_lora_args
    print( "\n2. Testing _parse_lora_args..." )
    try:
        # Create a minimal expeditor for testing parse method
        class MockConfig:
            def get( self, key, **kwargs ):
                return "test"
        expeditor = RuntimeArgumentExpeditor.__new__( RuntimeArgumentExpeditor )
        expeditor.debug = False

        # Test various formats
        result = expeditor._parse_lora_args( 'topic="quantum computing" budget=10' )
        assert result[ "topic" ] == "quantum computing", f"Expected 'quantum computing', got '{result.get( 'topic' )}'"
        assert result[ "budget" ] == "10"
        print( "   ✓ Double-quoted args parsed" )

        result = expeditor._parse_lora_args( "topic='AI safety'" )
        assert result[ "topic" ] == "AI safety"
        print( "   ✓ Single-quoted args parsed" )

        result = expeditor._parse_lora_args( "budget=50" )
        assert result[ "budget" ] == "50"
        print( "   ✓ Unquoted args parsed" )

        result = expeditor._parse_lora_args( "" )
        assert result == {}
        print( "   ✓ Empty string returns empty dict" )

        result = expeditor._parse_lora_args( None )
        assert result == {}
        print( "   ✓ None returns empty dict" )

        tests_passed += 1
    except Exception as e:
        print( f"   ✗ Failed: {e}" )
        tests_failed += 1

    # Test 3: Registry lookup
    print( "\n3. Testing registry lookup..." )
    try:
        entry = AGENTIC_AGENTS.get( "agent router go to deep research" )
        assert entry is not None
        assert "query" in entry[ "required_user_args" ]
        print( "   ✓ Deep research registry entry found" )

        entry = AGENTIC_AGENTS.get( "agent router go to podcast generator" )
        assert entry is not None
        assert "research" in entry[ "required_user_args" ]
        assert entry[ "special_handlers" ][ "research" ] == "fuzzy_file_match"
        print( "   ✓ Podcast generator registry entry found (with special handler)" )

        tests_passed += 1
    except Exception as e:
        print( f"   ✗ Failed: {e}" )
        tests_failed += 1

    # Test 4: CLI help capture
    print( "\n4. Testing CLI help capture..." )
    try:
        help_text = get_cli_help( "agent router go to deep research" )
        if help_text:
            print( f"   ✓ Deep research help captured ({len( help_text )} chars)" )
        else:
            print( "   ⚠ Help returned None (CLI module may not be runnable)" )

        help_none = get_cli_help( "nonexistent" )
        assert help_none is None
        print( "   ✓ Missing command returns None" )

        tests_passed += 1
    except Exception as e:
        print( f"   ✗ Failed: {e}" )
        tests_failed += 1

    # Test 5: ExpeditorResponse XML round-trip
    print( "\n5. Testing ExpeditorResponse XML round-trip..." )
    try:
        response = ExpeditorResponse(
            all_required_met = "false",
            args_present     = "query=test topic",
            args_missing     = "budget, audience"
        )

        assert not response.is_complete()
        assert response.get_missing_list() == [ "budget", "audience" ]
        assert response.get_present_dict() == { "query": "test topic" }

        xml = response.to_xml()
        parsed = ExpeditorResponse.from_xml( xml )
        assert parsed.all_required_met == "false"
        assert parsed.args_present == "query=test topic"
        print( "   ✓ XML round-trip works" )

        complete = ExpeditorResponse(
            all_required_met = "true",
            args_present     = "query=biodiversity",
            args_missing     = ""
        )
        assert complete.is_complete()
        assert complete.get_missing_list() == []
        print( "   ✓ Complete response detected correctly" )

        tests_passed += 1
    except Exception as e:
        print( f"   ✗ Failed: {e}" )
        tests_failed += 1

    # Summary
    print( f"\n{'=' * 60}" )
    print( f"Expeditor Smoke Test: {tests_passed} passed, {tests_failed} failed" )
    print( "=" * 60 )

    return tests_failed == 0


if __name__ == "__main__":
    success = quick_smoke_test()
    exit( 0 if success else 1 )
